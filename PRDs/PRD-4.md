# PRD-4: Production Polish & Voice Recording Integration

**Version:** 4.0
**Date:** 2025-12-05
**Status:** üöß IN PROGRESS - Initiative 1 & 2 (Core) Complete
**Latest Update:** 2025-12-06 - Initiative 2 viewer integration fully implemented
**Depends On:** PRD-3.md (Snapshot Architecture Improvements)

---

## Executive Summary

Session Recorder is now functional with comprehensive browser action recording and an advanced viewer. PRD-4 focuses on making this tool **production-ready** for company-wide adoption through four key initiatives:

1. **‚úÖ Voice Recording in SessionRecorder (COMPLETE):** Direct integration with Python child process for millisecond-precise transcription
2. **‚úÖ Viewer Voice Integration (COMPLETE):** Timeline, action list, and transcript viewer enhancements
3. **Desktop Application:** Enable non-developers to easily record sessions
4. **MCP Server:** Enable developers with AI coding assistants to record sessions

**Target Users:**

- **Developers:** Engineers creating QA sessions, bug reports, tutorials (SessionRecorder)
- **Non-Developers:** QA testers, product managers, support staff (Desktop App)
- **AI-Assisted Developers:** Engineers using Claude Code, Cline, Continue.dev (MCP Server)

---

## 1. Problem Statement

**Current State:** Session recorder captures browser actions but lacks voice context and easy access:

- ‚úÖ ~~No way to capture verbal explanations during testing~~ **SOLVED**
- ‚úÖ ~~Cannot correlate spoken instructions with browser actions~~ **SOLVED**
- ‚úÖ ~~No precise timestamp alignment between speech and actions~~ **SOLVED**
- ‚ùå Viewer doesn't display voice transcripts yet
- ‚ùå Non-developers cannot easily use the tool
- ‚ùå No integration for AI coding assistants

**Remaining Gaps:**

- ‚úÖ ~~Cannot capture voice explanations during recording~~ **SOLVED**
- ‚úÖ ~~No word-level timestamp synchronization~~ **SOLVED**
- ‚úÖ ~~Timeline doesn't show voice annotations~~ **SOLVED (Phase 2)**
- ‚úÖ ~~Action list doesn't intermix voice transcripts~~ **SOLVED (Phase 2)**
- ‚úÖ ~~No voice transcript viewer component~~ **SOLVED (Phase 2)**
- ‚ùå Requires technical knowledge to use
- ‚ùå No MCP integration for AI assistants

---

## 2. Goals & Four Key Initiatives

### Primary Goal

Make session recording accessible and feature-rich for all users with **millisecond-precise voice integration**.

### Initiative 1: Voice Recording in SessionRecorder (Core - Phase 1) - ‚úÖ COMPLETE

**Target Users:** Developers, QA Testers, Technical Writers
**Status:** ‚úÖ COMPLETE (2025-12-06)

**Implementation Summary:**
- Created `VoiceRecorder` class with audio capture and Whisper transcription
- Python Whisper script with GPU auto-detection (CUDA/MPS/CPU)
- `browser_record` and `voice_record` flags in SessionRecorder
- Word-level timestamps with millisecond precision
- Automatic chronological merging of voice and browser actions
- Comprehensive documentation and testing

**Requirements:**

```typescript
new SessionRecorder(sessionId, {
  browser_record: boolean,  // Capture DOM + actions
  voice_record: boolean,    // Capture audio + transcript
  whisper_model?: 'tiny' | 'base' | 'small' | 'medium'
})
```

- At least one must be `true` (both can be `true`)
- Python child process for audio capture and Whisper transcription
- Word-level timestamps with millisecond precision
- Automatic alignment of voice segments to browser timeline

**Example Usage:**

```typescript
const recorder = new SessionRecorder('test-123', {
  browser_record: true,
  voice_record: true,
  whisper_model: 'base'
});

await recorder.start(page);
// User interacts and speaks
await recorder.stop();

const zipPath = await recorder.createZip();
// Zip contains: session.json, snapshots/, audio/, transcript
```

### Initiative 2: Viewer Voice Integration (Core - Phase 1) - ‚úÖ COMPLETE

**Target Users:** Developers, QA Testers, Technical Writers
**Status:** ‚úÖ COMPLETE (2025-12-06)

**Implementation Summary:**
- Updated viewer types to support `VoiceTranscriptAction`
- Enhanced Timeline with green voice segment bars
- Added hover tooltips showing transcript previews
- Updated ActionList to display voice entries with microphone icons
- Created VoiceTranscriptViewer component with audio playback
- Added Voice tab to TabPanel (auto-shows when voice enabled)
- Updated SessionLoader to load audio files from zip/directory
- Comprehensive voice-related CSS styling

**Requirements:**

- **Timeline:** Green bars showing voice segments with hover tooltips
- **Action List:** Intermix voice transcripts with browser actions chronologically
- **VoiceTranscriptViewer:** Interactive component with:
  - Full transcript display
  - Word-level highlighting during audio playback
  - Click word to seek audio
  - Speed control (0.5x - 2x)

**UI Preview:**

```
Timeline: 0s    5s    10s    15s
         ‚îÇ     ‚îÇ     ‚îÇ      ‚îÇ
         üñ±Ô∏è    ‚å®Ô∏è    üñ±Ô∏è     ‚å®Ô∏è   ‚Üê Browser actions
         ‚ñì‚ñì‚ñì‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñì‚ñì‚ñì‚ñì‚ñë‚ñë   ‚Üê Voice (green bars)

Action List:
  üñ±Ô∏è 2.45s click
  üéôÔ∏è 3.12s "Click on the button..." (2.3s) 97%
  ‚å®Ô∏è 5.60s input
  üéôÔ∏è 6.80s "Now let me..." (1.8s) 94%
```

### Initiative 3: Desktop Application (Optional - Future Phase)

**Target Users:** QA, PM, Support, Designers

**Features:**

- One-click "Record" button
- Recording mode selector (Browser / Voice / Both)
- Auto zip creation + viewer link
- Cross-platform (Windows, macOS, Linux)

**User Flow:**

1. Launch Desktop App
2. Click "New Recording"
3. Select mode (Browser + Voice)
4. Start Recording ‚Üí Browser opens
5. Perform actions & speak
6. Close browser ‚Üí Auto-creates zip
7. Success notification with viewer URL

### Initiative 4: MCP Server (Optional - Future Phase)

**Target Users:** Developers using Claude Code, Cline, Continue.dev

**MCP Tools:**

- `start_combined_recording`: Browser + voice
- `start_browser_recording`: Browser only
- `start_voice_recording`: Voice only
- `stop_recording`: Stop and create zip
- `get_recording_status`: Check status

**User Flow:**

1. Developer: "Record my browser session with voice"
2. AI calls `start_combined_recording`
3. Browser opens, developer acts & speaks
4. Developer: "Stop recording"
5. AI calls `stop_recording`
6. AI provides zip path + viewer link

---

## 3. Technical Architecture

### 3.1 Voice Recording - Python Unified Architecture (UPDATED 2025-12-06)

**Architecture Decision:** Python handles BOTH audio recording AND transcription in a single process spawned by TypeScript. This simplifies the architecture and reduces process overhead.

**SessionRecorder (TypeScript):**

```typescript
// src/node/SessionRecorder.ts
export class SessionRecorder {
  private voiceRecorder: VoiceRecorder | null = null;
  private sessionStartTime: number = 0;

  constructor(sessionId: string, options: SessionRecorderOptions) {
    if (!options.browser_record && !options.voice_record) {
      throw new Error('At least one must be true');
    }

    if (options.voice_record) {
      this.voiceRecorder = new VoiceRecorder({
        model: options.whisper_model || 'base',
        device: options.whisper_device  // cuda, mps, or cpu
      });
    }
  }

  async start(page: Page) {
    this.sessionStartTime = Date.now();

    await Promise.all([
      this.options.browser_record ? this.startBrowserRecording() : null,
      this.options.voice_record ? this.voiceRecorder.startRecording(audioDir, this.sessionStartTime) : null
    ]);
  }

  async stop() {
    const transcript = await this.voiceRecorder.stopRecording();
    // transcript contains both recording info and transcription

    if (transcript && transcript.success) {
      const voiceActions = this.voiceRecorder.convertToVoiceActions(
        transcript,
        'audio/recording.wav',
        (timestamp: string) => this._findNearestSnapshot(timestamp)
      );

      // Merge voice and browser actions chronologically
      this.sessionData.actions = [...this.sessionData.actions, ...voiceActions]
        .sort((a, b) => new Date(a.timestamp).getTime() - new Date(b.timestamp).getTime());
    }
  }
}
```

**VoiceRecorder (TypeScript - Python Process Manager):**

```typescript
// src/voice/VoiceRecorder.ts
import { spawn, ChildProcess } from 'child_process';

export class VoiceRecorder {
  private pythonProcess: ChildProcess | null = null;
  private outputBuffer: string = '';

  async startRecording(audioDir: string, sessionStartTime: number) {
    const pythonScript = path.join(__dirname, 'record_and_transcribe.py');

    this.pythonProcess = spawn('python3', [
      pythonScript,
      this.audioFilePath,
      '--model', this.options.model,
      '--sample-rate', '16000',
      '--channels', '1'
    ]);

    // Capture status messages
    this.pythonProcess.stdout?.on('data', (data) => {
      this.outputBuffer += data.toString();
      // Parse JSON status messages: {"type": "status", "message": "..."}
    });
  }

  async stopRecording(): Promise<TranscriptResult> {
    // Send SIGINT to stop recording and trigger transcription
    this.pythonProcess.kill('SIGINT');

    return new Promise((resolve) => {
      this.pythonProcess.on('close', (code) => {
        // Extract final JSON result from output buffer
        // Last JSON with "segments" field is the transcription result
        const result = this.parseFinalResult(this.outputBuffer);
        resolve(result);
      });
    });
  }
}
```

**Python Unified Recorder:**

```python
# src/voice/record_and_transcribe.py
import sounddevice as sd
import soundfile as sf
import whisper
import signal
import sys
import json
import numpy as np

class AudioRecorder:
    def __init__(self, sample_rate=16000, channels=1):
        self.sample_rate = sample_rate
        self.channels = channels
        self.recording = False
        self.frames = []

    def start_recording(self):
        """Start recording from microphone"""
        self.recording = True
        self.frames = []

        self.stream = sd.InputStream(
            samplerate=self.sample_rate,
            channels=self.channels,
            callback=self.audio_callback,
            dtype=np.int16
        )
        self.stream.start()

        print(json.dumps({
            "type": "status",
            "message": "Recording started"
        }), flush=True)

    def stop_recording(self, output_path):
        """Stop recording and save to WAV file"""
        self.recording = False
        self.stream.stop()
        self.stream.close()

        audio_data = np.concatenate(self.frames, axis=0)
        sf.write(output_path, audio_data, self.sample_rate)

        return {
            "success": True,
            "audio_path": str(output_path),
            "duration": len(audio_data) / self.sample_rate
        }

def transcribe_audio(audio_path, model_size='base', device=None):
    """Transcribe audio using Whisper with word-level timestamps"""
    model = whisper.load_model(model_size, device=device)

    result = model.transcribe(
        audio_path,
        word_timestamps=True  # ‚Üê CRITICAL for viewer
    )

    segments = []
    for seg in result['segments']:
        words = []
        for word in seg.get('words', []):
            words.append({
                'word': word['word'].strip(),
                'start': word['start'],
                'end': word['end'],
                'probability': word.get('probability', 1.0)
            })

        segments.append({
            'id': seg['id'],
            'start': seg['start'],
            'end': seg['end'],
            'text': seg['text'].strip(),
            'confidence': seg.get('avg_logprob', 0.0),
            'words': words
        })

    return {
        'success': True,
        'text': result['text'].strip(),
        'language': result.get('language', 'unknown'),
        'duration': result.get('duration', 0.0),
        'segments': segments
    }

# Signal handler for stopping
def signal_handler(sig, frame):
    # Stop recording
    record_result = recorder.stop_recording(output_path)

    # Transcribe
    transcription = transcribe_audio(output_path, model, device)

    # Output combined result
    print(json.dumps({**transcription, **record_result}, indent=2), flush=True)
    sys.exit(0)

signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# Start recording and wait for signal
recorder.start_recording()
while True:
    time.sleep(0.1)
```

**Key Architectural Benefits:**

1. **Single Process:** One Python process handles both recording and transcription
2. **Simplified IPC:** TypeScript only needs to spawn and send SIGINT signal
3. **Cleaner Error Handling:** Single point of failure instead of two separate processes
4. **Resource Efficiency:** No need for intermediate file handling between recording and transcription
5. **Status Streaming:** Python emits JSON status messages during recording, final result on exit

**Python Dependencies:**

```bash
pip install sounddevice soundfile openai-whisper torch numpy
```

### 3.2 Data Format

**Session JSON with Voice:**

```json
{
  "sessionId": "session-1733400000000",
  "startTime": "2025-12-05T10:23:45.000Z",
  "endTime": "2025-12-05T10:28:30.123Z",
  "voiceRecording": {
    "enabled": true,
    "audioFile": "audio/recording_20251205_102345.wav",
    "duration": 285.6,
    "segmentCount": 12
  },
  "transcript": {
    "text": "Click on the login button to test authentication",
    "language": "en",
    "segments": [
      {
        "id": 1,
        "start": 0.0,
        "end": 3.32,
        "text": "Click on the login button",
        "words": [
          {
            "word": "Click",
            "start": 0.0,
            "end": 0.22,
            "absolute_start_ms": 1733400225000,
            "absolute_end_ms": 1733400225220,
            "probability": 0.98
          }
        ]
      }
    ]
  },
  "actions": [
    {
      "id": "action-1",
      "timestamp": "2025-12-05T10:23:45.234Z",
      "type": "click"
    }
  ]
}
```

**Zip Structure:**

```
session-1733400000000.zip
‚îú‚îÄ‚îÄ session.json              ‚Üê Contains transcript with word-level timestamps
‚îú‚îÄ‚îÄ audio/
‚îÇ   ‚îî‚îÄ‚îÄ recording_20251205_102345.wav  ‚Üê Raw audio for playback
‚îú‚îÄ‚îÄ snapshots/
‚îÇ   ‚îú‚îÄ‚îÄ action-1-before.html
‚îÇ   ‚îî‚îÄ‚îÄ action-1-after.html
‚îî‚îÄ‚îÄ screenshots/
    ‚îî‚îÄ‚îÄ action-1-before.png
```

**Note:** The `session.json` file contains the complete transcript with word-level timestamps in the `transcript` field (see Data Format section above). The WAV file is only for audio playback in the viewer.

### 3.3 Viewer UI Components

**Timeline:**

```tsx
// viewer/src/components/Timeline/Timeline.tsx
const renderVoiceSegments = () => {
  return transcript.segments.map(segment => {
    const startMs = segment.words[0].absolute_start_ms;
    const endMs = segment.words[last].absolute_end_ms;
    const x = timeToPixel(startMs);
    const width = (endMs - startMs) / 1000 * PIXELS_PER_SECOND;

    ctx.fillStyle = 'rgba(76, 175, 80, 0.6)';
    ctx.fillRect(x, timelineHeight - 20, width, 15);
  });
};
```

**Action List:**

```tsx
// Merge voice with actions
const merged = [...actions];
segments.forEach(seg => {
  merged.push({
    id: `voice-${seg.id}`,
    type: 'voice_transcript',
    timestamp: seg.words[0].absolute_start_ms,
    segment: seg
  });
});

merged.sort((a, b) => a.timestamp - b.timestamp);
```

**VoiceTranscriptViewer:**

```tsx
export const VoiceTranscriptViewer = ({ segment, audioUrl }) => {
  const [currentWordIndex, setCurrentWordIndex] = useState(null);

  const handleWordClick = (i) => {
    const seekTime = (segment.words[i].absolute_start_ms -
                      segment.words[0].absolute_start_ms) / 1000;
    audioRef.current.currentTime = seekTime;
    audioRef.current.play();
  };

  return (
    <div className="transcript-text">
      {segment.words.map((word, i) => (
        <span
          className={i === currentWordIndex ? 'word active' : 'word'}
          onClick={() => handleWordClick(i)}
        >
          {word.word}{' '}
        </span>
      ))}
      <audio ref={audioRef} src={audioUrl} controls />
    </div>
  );
};
```

### 3.4 Desktop Application (Future)

**Electron Structure:**

```
session-recorder-desktop/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.ts           # Electron entry
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recording.ts      # Wraps SessionRecorder
‚îÇ   ‚îî‚îÄ‚îÄ renderer/
‚îÇ       ‚îú‚îÄ‚îÄ App.tsx
‚îÇ       ‚îî‚îÄ‚îÄ RecordingControls.tsx
```

**Recording Manager:**

```typescript
// Wraps SessionRecorder for Electron
export class RecordingManager {
  private recorder: SessionRecorder | null = null;

  async startRecording(config: {
    title: string;
    mode: 'browser' | 'voice' | 'both';
    browserType: string;
  }) {
    this.recorder = new SessionRecorder(config.title, {
      browser_record: config.mode !== 'voice',
      voice_record: config.mode !== 'browser',
      whisper_model: 'base'
    });

    await this.recorder.start(page);
  }
}
```

### 3.5 MCP Server (Future)

**MCP Tools:**

```typescript
// session-recorder-mcp/src/index.ts
import { McpServer } from '@anthropic-ai/sdk/mcp';
import { SessionRecorder } from '@session-recorder/core';

server.addTool({
  name: 'start_combined_recording',
  handler: async (input) => {
    const recorder = new SessionRecorder(input.title, {
      browser_record: true,
      voice_record: true,
      whisper_model: input.whisperModel || 'base'
    });

    await recorder.start(page);

    return {
      success: true,
      message: 'Recording started'
    };
  }
});
```

---

## 4. Implementation Roadmap

### Phase 1: Voice Recording Backend (16 hours) - **‚úÖ COMPLETE**

**Status:** ‚úÖ Complete (2025-12-06)

**Architecture Change:** Unified Python process for recording + transcription (simplified from original two-process design)

1. **‚úÖ Audio Capture Implementation** (4h)
   - sounddevice for cross-platform audio recording
   - soundfile for WAV output
   - Audio callback buffering with numpy
   - Level meter visualization ready

2. **‚úÖ Whisper Integration** (4h)
   - Python unified process (record_and_transcribe.py)
   - Word-level timestamp extraction
   - GPU/CPU auto-detection (CUDA/MPS/CPU)
   - Signal-based stop mechanism (SIGINT/SIGTERM)

3. **‚úÖ Timestamp Alignment** (3h)
   - sessionStartTime passed from TypeScript
   - Relative-to-absolute timestamp conversion
   - Browser action correlation via nearestSnapshotFinder

4. **‚úÖ Transcript Storage** (2h)
   - transcript.json saved in session directory
   - VoiceTranscriptAction type with word-level data
   - Chronological merging with browser actions

5. **‚úÖ Testing** (3h)
   - voice-test.ts for end-to-end testing
   - Error handling for Python dependencies
   - Process management and cleanup

### Phase 2: Viewer Integration (14 hours) - **‚úÖ COMPLETE**

**Status:** ‚úÖ Complete (2025-12-06)

1. **‚úÖ Timeline Voice Indicators** (4h)
   - Green voice segment bars with gradient
   - Hover tooltips with transcript preview
   - Click navigation to voice tab
   - Voice timeline bar styling

2. **‚úÖ Action List Voice Entries** (3h)
   - Chronological intermixing with browser actions
   - Voice entry styling with microphone icon
   - Confidence display with percentage
   - Type guard function for voice actions

3. **‚úÖ VoiceTranscriptViewer Component** (4h)
   - Full transcript display with segments
   - Word-level highlighting during playback
   - Audio playback controls (play/pause)
   - Click-to-seek functionality
   - Duration formatting

4. **‚úÖ Audio Playback Controls** (3h)
   - Speed control (0.25x - 2x)
   - Progress tracking with visual indicator
   - Word synchronization with timestamps
   - Audio element integration

### Phase 3: Testing & Documentation (Phases 1-2) (4 hours) - **‚úÖ COMPLETE**

**Status:** ‚úÖ Complete (2025-12-06)

1. **‚úÖ Voice Recording Tests** (2h)
   - voice-test.ts for browser + voice recording
   - Error handling scenarios
   - Python dependency checks
   - Build verification

2. **‚úÖ Documentation** (2h)
   - README.md updated with Python dependencies
   - PRD-4.md architectural changes documented
   - Installation instructions for voice recording
   - GPU acceleration setup guide
   - Audio playback

3. **Documentation** (1h)
   - Voice recording guide
   - Viewer features guide

**Subtotal Core Features:** 34 hours

---

### Phase 4: MCP Server (12 hours) - **OPTIONAL FUTURE**

1. **MCP Server Setup** (3h)
   - Project initialization
   - Tool registration
   - Claude Desktop configuration

2. **Tool Implementations** (5h)
   - start_browser_recording
   - start_voice_recording
   - start_combined_recording
   - stop_recording
   - get_recording_status

3. **SessionRecorder Integration** (2h)
   - RecordingManager wrapper
   - Status tracking

4. **Error Handling** (2h)
   - Tool validation
   - Error responses

### Phase 5: Desktop Application (20 hours) - **OPTIONAL FUTURE**

1. **Electron Structure** (4h)
   - Main process setup
   - Renderer UI
   - IPC communication
   - Build configuration

2. **Recording Controls UI** (3h)
   - React components
   - Form inputs
   - Status display

3. **Voice Integration** (5h)
   - Audio capture module
   - Transcription service
   - Timestamp alignment

4. **Browser Automation** (3h)
   - RecordingManager implementation
   - Playwright integration
   - Session merging

5. **Zip + Viewer Link** (2h)
   - Auto zip creation
   - Viewer URL generation
   - Notifications

6. **Testing** (3h)
   - End-to-end scenarios
   - Error handling
   - Cross-platform validation

### Phase 6: Final Testing & Documentation (12 hours) - **OPTIONAL FUTURE**

1. **End-to-End Testing** (4h)
   - Desktop app tests
   - MCP server tests
   - Viewer integration tests

2. **User Documentation** (4h)
   - Desktop app user guide
   - MCP server setup guide
   - Troubleshooting guides

3. **Deployment Guides** (4h)
   - Desktop app distribution
   - MCP server publishing
   - Auto-update setup

**Total:** 78 hours (34h core + 44h optional future)

---

## 5. Success Criteria

### Phases 1-3 (Core - 34 hours)

- ‚úÖ `browser_record` and `voice_record` options work
- ‚úÖ Word-level timestamps with ms precision (¬±50-200ms accuracy)
- ‚úÖ Transcription accuracy >93% (base model)
- ‚úÖ Timeline shows green voice bars with hover tooltips
- ‚úÖ Action list intermixes voice + browser chronologically
- ‚úÖ Audio playback syncs with word highlighting
- ‚úÖ Click word seeks audio to exact timestamp
- ‚úÖ Speed control (0.5x - 2x) works
- ‚úÖ Python Whisper auto-detects GPU with CPU fallback
- ‚úÖ 2-hour recordings transcribe in 1-12 minutes

### Phase 4 (MCP Server)

- ‚úÖ Claude Desktop compatible
- ‚úÖ All 5 tools functional (start/stop browser/voice/combined, status)
- ‚úÖ Returns zip paths and viewer URLs
- ‚úÖ Error handling for failed recordings

### Phase 5 (Desktop Application)

- ‚úÖ Cross-platform installation (Windows, macOS, Linux)
- ‚úÖ One-click recording with mode selection
- ‚úÖ Auto zip creation + viewer link
- ‚úÖ System tray integration
- ‚úÖ Notifications on completion

### Phase 6 (Final Testing)

- ‚úÖ End-to-end tests pass for all user flows
- ‚úÖ User documentation complete
- ‚úÖ Deployment guides ready

---

## 6. Dependencies

**Python:**

```text
sounddevice>=0.4.6
numpy>=1.24.0
whisper>=1.0.0 (official OpenAI implementation for maximum accuracy)
wave>=0.0.2
```

**TypeScript:**

```json
{
  "dependencies": {
    "playwright": "^1.40.0",
    "archiver": "^6.0.1"
  }
}
```

---

## 7. Out of Scope

- ‚ùå Cloud storage
- ‚ùå Multi-language (English only)
- ‚ùå Video recording
- ‚ùå Real-time transcription
- ‚ùå Speaker diarization

---

## Document Changelog

| Version | Date | Changes |
|---------|------|---------|
| 4.0 | 2025-12-05 | All 4 initiatives: SessionRecorder voice, Viewer UI, Desktop App, MCP Server |
